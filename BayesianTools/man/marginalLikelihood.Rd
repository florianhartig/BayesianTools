% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/marginalLikelihood.R
\name{marginalLikelihood}
\alias{marginalLikelihood}
\title{Calcluated the marginal likelihood from a set of MCMC samples}
\usage{
marginalLikelihood(sampler, numSamples = 1000, method = "Chib", ...)
}
\arguments{
\item{sampler}{an object that implements the getSample function, i.e. a mcmc / smc Sampler (list)}

\item{numSamples}{number of samples to use. How this works, and if it requires recalculating the likelihood, depends on the method}

\item{method}{method to choose. Currently available are "Chib" (default), the harmonic mean "HM", sampling from the prior "prior", and bridge sampling "Bridge". See details}

\item{...}{further arguments passed to \code{\link{getSample}}}
}
\value{
A list with log of the marginal likelihood, as well as other diagnostics depending on the chose method
}
\description{
Calcluated the marginal likelihood from a set of MCMC samples
}
\details{
The marginal likelihood is the average likelihood across the prior space. It is used, for example, for Bayesian model selection and model averaging. 

It is defined as \deqn{ML = \int L(\Theta) p(\Theta) d\Theta}

Given that MLs are calculated for each model, you can get posterior weights (for model selection and/or model averaging) on the model by 

\deqn{P(M_i|D) = ML_i * p(M_i) / (\sum_i ML_i * p(M_i) )}

In BT, we return the log ML, so you will have to exp all values for this formula. 

It is well-known that the ML is VERY dependent on the prior, and in particular the choice of the width of uninformative priors may have major impacts on the relative weights of the models. It has therefore been suggested to not use the ML for model averaging / selection on uninformative priors. If you have no informative priors, and option is to split the data into two parts, use one part to generate informative priors for the model, and the second part for the model selection. See Dormann et al., 2018, in particular the Appendix, for an example. 

Be aware that marginal likelihood calculations are notoriously prone to numerical stability issues. Especially in high-dimensional parameter spaces, there is no guarantee that any of the implemented algorithms will converge reasonably fast.


The marginalLikelihood function currently implements four ways to calculate the marginal likelihood. The recommended (and default) method is the method "Chib" (Chib and Jeliazkov, 2001), which is based on MCMC samples, with a limited number of additional calculations. Despite being the current recommendation, note there are some numeric issues with this algorithm that may limit reliability for larger dimensions.
  
 The harmonic mean approximation, is implemented only for comparison. Note that the method is numerically unrealiable and usually should not be used. \cr

The third method is simply sampling from the prior. While in principle unbiased, it will only converge for a large number of samples, and is therefore numerically inefficient. 

The Bridge method uses bridge sampling as implemented in the R package "bridgesampling". It is potentially more exact than the Chib method, but might require more computation time. However, this may be very dependent on the sampler.
}
\examples{
# Low dimensional case with narrow priors - all methods have low error

# we use a truncated normal for the likelihood to make sure that the density 
# integrates to 1 - makes it easier to calcuate the theoretical ML
likelihood <- function(x) sum(msm::dtnorm(x, log = TRUE, lower = -1, upper = 1))
prior = createUniformPrior(lower = rep(-1,2), upper = rep(1,2))
bayesianSetup <- createBayesianSetup(likelihood = likelihood, prior = prior)
out = runMCMC(bayesianSetup = bayesianSetup, settings = list(iterations = 5000))

# plot(out)

# theoretical value
theory = log(1/(2^2))

marginalLikelihood(out)$ln.ML - theory
marginalLikelihood(out, method = "Prior", numSamples =  500)$ln.ML - theory
marginalLikelihood(out, method = "HM", numSamples =  500)$ln.ML - theory
marginalLikelihood(out, method = "Bridge", numSamples =  500)$ln.ML - theory


# higher dimensions - wide prior - HM and bridge don't work.

likelihood <- function(x) sum(msm::dtnorm(x, log = TRUE, lower = -10, upper = 10))
prior = createUniformPrior(lower = rep(-10,3), upper = rep(10,3))
bayesianSetup <- createBayesianSetup(likelihood = likelihood, prior = prior)
out = runMCMC(bayesianSetup = bayesianSetup, settings = list(iterations = 5000))

# plot(out)

# theoretical value
theory = log(1/(20^3))

marginalLikelihood(out)$ln.ML - theory
marginalLikelihood(out, method = "Prior", numSamples =  500)$ln.ML - theory
marginalLikelihood(out, method = "HM", numSamples =  500)$ln.ML - theory
marginalLikelihood(out, method = "Bridge", numSamples =  500)$ln.ML - theory



}
\references{
Chib, Siddhartha, and Ivan Jeliazkov. "Marginal likelihood from the Metropolis-Hastings output." Journal of the American Statistical Association 96.453 (2001): 270-281.

Dormann et al. 2018. Model averaging in ecology: a review of Bayesian, information-theoretic, and tactical approaches for predictive inference. Ecological Monographs
}
\seealso{
\code{\link{WAIC}}, \code{\link{DIC}}, \code{\link{MAP}}
}
\author{
Florian Hartig
}
